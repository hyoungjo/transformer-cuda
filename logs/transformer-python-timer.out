[SLURM][INFO] Run started on baram at Sat Dec 20 03:22:23 AM KST 2025
[PYTHON][INFO] Running on Device: cuda
[PYTHON][INFO] Loading tokenizer and model
[PYTHON][INFO] Loading texts
[PYTHON][INFO] Running torch.utils.benchmark...
<torch.utils.benchmark.utils.common.Measurement object at 0x7c5ba829ce60>
Inference: seq_len = 13
Text 0
  Median: 3.75 ms
  IQR:    0.04 ms (3.73 to 3.77)
  132 measurements, 1 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x7c5b8de17fb0>
Inference: seq_len = 17
Text 1
  Median: 2.69 ms
  IQR:    0.04 ms (2.68 to 2.72)
  183 measurements, 1 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x7c5b8d359550>
Inference: seq_len = 64
Text 2
  Median: 2.80 ms
  IQR:    0.26 ms (2.62 to 2.88)
  181 measurements, 1 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x7c5b8d9b3770>
Inference: seq_len = 200
Text 3
  Median: 3.68 ms
  IQR:    0.05 ms (3.66 to 3.71)
  136 measurements, 1 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x7c5be85619d0>
Inference: seq_len = 301
Text 4
  Median: 3.57 ms
  IQR:    0.04 ms (3.56 to 3.60)
  140 measurements, 1 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x7c5b8cecf6e0>
Inference: seq_len = 305
Text 5
  Median: 3.57 ms
  IQR:    0.02 ms (3.56 to 3.59)
  140 measurements, 1 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x7c5ba862e840>
Inference: seq_len = 320
Text 6
  Median: 3.72 ms
  IQR:    0.02 ms (3.71 to 3.73)
  134 measurements, 1 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x7c5b8d4dc6b0>
Inference: seq_len = 332
Text 7
  Median: 3.84 ms
  IQR:    0.05 ms (3.83 to 3.88)
  130 measurements, 1 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x7c5c1421de20>
Inference: seq_len = 342
Text 8
  Median: 4.01 ms
  IQR:    0.02 ms (4.00 to 4.03)
  124 measurements, 1 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x7c5b8cedaf60>
Inference: seq_len = 344
Text 9
  Median: 4.02 ms
  IQR:    0.02 ms (4.01 to 4.03)
  125 measurements, 1 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x7c5b8c5125a0>
Inference: seq_len = 363
Text 10
  Median: 3.97 ms
  IQR:    0.02 ms (3.96 to 3.98)
  126 measurements, 1 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x7c5b8c9505f0>
Inference: seq_len = 364
Text 11
  Median: 3.99 ms
  IQR:    0.05 ms (3.97 to 4.02)
  125 measurements, 1 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x7c5be82bd220>
Inference: seq_len = 367
Text 12
  Median: 3.99 ms
  IQR:    0.02 ms (3.98 to 3.99)
  126 measurements, 1 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x7c5b8cafc290>
Inference: seq_len = 371
Text 13
  Median: 4.00 ms
  IQR:    0.02 ms (3.99 to 4.01)
  125 measurements, 1 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x7c5b8ce0d3d0>
Inference: seq_len = 732
Text 14
  Median: 8.62 ms
  IQR:    0.02 ms (8.61 to 8.63)
  58 measurements, 1 runs per measurement, 1 thread
[SLURM][INFO] Run finished at Sat Dec 20 03:24:47 AM KST 2025
