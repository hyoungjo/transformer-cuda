[SLURM][INFO] Run started on baram at Sat Nov 29 12:47:05 AM KST 2025
[PYTHON][INFO] Running on Device: cuda
[PYTHON][INFO] Loading tokenizer and model
[PYTHON][INFO] Loading texts
[PYTHON][INFO] Running torch.utils.benchmark...
<torch.utils.benchmark.utils.common.Measurement object at 0x71204d9b5dc0>
Inference: seq_len = 6
Text 0
  Median: 8.22 ms
  IQR:    0.10 ms (8.19 to 8.28)
  61 measurements, 1 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204d9b5280>
Inference: seq_len = 8
Text 1
  Median: 7.93 ms
  IQR:    0.00 ms (7.93 to 7.93)
  7 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204e0665a0>
Inference: seq_len = 13
Text 2
  Median: 8.02 ms
  IQR:    0.02 ms (8.00 to 8.02)
  7 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204d9ec080>
Inference: seq_len = 10
Text 3
  Median: 7.92 ms
  IQR:    0.04 ms (7.90 to 7.94)
  7 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204d8b5b20>
Inference: seq_len = 7
Text 4
  Median: 7.90 ms
  IQR:    0.02 ms (7.88 to 7.90)
  7 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204d9b5dc0>
Inference: seq_len = 10
Text 5
  Median: 5.43 ms
  IQR:    0.80 ms (5.41 to 6.21)
  9 measurements, 10 runs per measurement, 1 thread
  WARNING: Interquartile range is 14.7% of the median measurement.
           This could indicate system fluctuation.
<torch.utils.benchmark.utils.common.Measurement object at 0x71204d9b5280>
Inference: seq_len = 13
Text 6
  Median: 5.47 ms
  IQR:    0.03 ms (5.47 to 5.50)
  10 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204da77140>
Inference: seq_len = 9
Text 7
  Median: 5.60 ms
  IQR:    0.12 ms (5.49 to 5.61)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204d9ec080>
Inference: seq_len = 14
Text 8
  Median: 5.60 ms
  IQR:    0.11 ms (5.52 to 5.63)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204d758ec0>
Inference: seq_len = 7
Text 9
  Median: 5.55 ms
  IQR:    0.36 ms (5.42 to 5.78)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204db9ca70>
Inference: seq_len = 11
Text 10
  6.01 ms
  1 measurement, 100 runs , 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204d9b5280>
Inference: seq_len = 7
Text 11
  Median: 5.59 ms
  IQR:    0.18 ms (5.52 to 5.70)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204dbfd010>
Inference: seq_len = 7
Text 12
  Median: 5.60 ms
  IQR:    0.13 ms (5.49 to 5.62)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204d8b5b20>
Inference: seq_len = 6
Text 13
  Median: 5.61 ms
  IQR:    0.07 ms (5.59 to 5.66)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204d9b5dc0>
Inference: seq_len = 10
Text 14
  Median: 5.57 ms
  IQR:    0.16 ms (5.49 to 5.65)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204de9f3e0>
Inference: seq_len = 7
Text 15
  Median: 5.48 ms
  IQR:    0.22 ms (5.45 to 5.67)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204ea29010>
Inference: seq_len = 8
Text 16
  Median: 5.51 ms
  IQR:    0.12 ms (5.49 to 5.61)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204dbfd010>
Inference: seq_len = 7
Text 17
  Median: 5.52 ms
  IQR:    0.04 ms (5.51 to 5.55)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204d8b5b20>
Inference: seq_len = 6
Text 18
  Median: 6.25 ms
  IQR:    0.58 ms (6.17 to 6.74)
  8 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x712060d37c50>
Inference: seq_len = 10
Text 19
  Median: 5.63 ms
  IQR:    0.20 ms (5.56 to 5.76)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204e0665a0>
Inference: seq_len = 10
Text 20
  Median: 5.54 ms
  IQR:    0.36 ms (5.47 to 5.83)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204ea29010>
Inference: seq_len = 6
Text 21
  Median: 5.52 ms
  IQR:    0.04 ms (5.49 to 5.54)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204dbc2720>
Inference: seq_len = 7
Text 22
  Median: 5.48 ms
  IQR:    0.18 ms (5.45 to 5.63)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204da77140>
Inference: seq_len = 8
Text 23
  Median: 5.85 ms
  IQR:    0.28 ms (5.61 to 5.89)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204db9ca70>
Inference: seq_len = 13
Text 24
  Median: 5.79 ms
  IQR:    0.12 ms (5.69 to 5.82)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204d9b5dc0>
Inference: seq_len = 17
Text 25
  Median: 5.40 ms
  IQR:    0.14 ms (5.34 to 5.48)
  10 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204de9f3e0>
Inference: seq_len = 64
Text 26
  Median: 5.72 ms
  IQR:    0.04 ms (5.70 to 5.74)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204e0665a0>
Inference: seq_len = 102
Text 27
  Median: 5.75 ms
  IQR:    0.25 ms (5.66 to 5.91)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204ea29010>
Inference: seq_len = 144
Text 28
  Median: 5.91 ms
  IQR:    0.06 ms (5.85 to 5.91)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204df05370>
Inference: seq_len = 162
Text 29
  Median: 5.84 ms
  IQR:    0.04 ms (5.83 to 5.87)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204d9b5dc0>
Inference: seq_len = 163
Text 30
  Median: 5.95 ms
  IQR:    0.17 ms (5.86 to 6.03)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204d9ec080>
Inference: seq_len = 200
Text 31
  Median: 6.11 ms
  IQR:    0.03 ms (6.10 to 6.13)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x712068e92db0>
Inference: seq_len = 180
Text 32
  Median: 5.88 ms
  IQR:    0.08 ms (5.86 to 5.94)
  9 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204db9ca70>
Inference: seq_len = 604
Text 33
  Median: 14.09 ms
  IQR:    0.02 ms (14.08 to 14.10)
  4 measurements, 10 runs per measurement, 1 thread
<torch.utils.benchmark.utils.common.Measurement object at 0x71204d758ec0>
Inference: seq_len = 732
Text 34
  Median: 16.53 ms
  IQR:    0.02 ms (16.51 to 16.54)
  4 measurements, 10 runs per measurement, 1 thread
[SLURM][INFO] Run finished at Sat Nov 29 12:47:36 AM KST 2025
